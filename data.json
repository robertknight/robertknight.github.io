{
  "config": {
    "inputDir": ".",
    "title": "Rob Knight",
    "outputDir": "/home/robert/projects/blog/robertknight.github.io/_site",
    "componentsDir": "/home/robert/projects/blog/robertknight.github.io/components",
    "rootUrl": "",
    "author": {
      "name": "Rob Knight",
      "photoUrl": "http://www.gravatar.com/avatar/217c7106a3d4eba3584cd38abf8ee107?s=128",
      "twitterId": "robknight_",
      "githubId": "robertknight",
      "email": "robertknight@gmail.com"
    }
  },
  "posts": [
    {
      "metadata": {
        "slug": "react-europe-2016",
        "title": "Notes on React Europe 2016",
        "date": "2016-06-04T00:00:00.000Z",
        "tags": [
          "conferences"
        ]
      },
      "body": "\nThe second [React Europe](https://www.react-europe.org/) conference has just finished and this is a jotting-down\nof some thoughts on the conference and my favorite talks whilst the Parisian red wine and almond croissants clear out of my system.\n\nThe organizers are kindly **[putting up videos on\nYouTube](https://www.youtube.com/channel/UCorlLn2oZfgOJ-FUcF2eZ1A)** as I write\nthis.\n\n## The Conference\n\nThis was a two-day conference preceded by a hackathon kindly hosted by Mozilla\nand a welcome event hosted by [Red Badger](https://twitter.com/redbadgerteam). It was well organized and ran smoothly, the main\npart consisted purely of 30 minute talks with a few lightning-talk spots. There\nwas plenty of time in-between for discussions.\n\nFor the hackathon attendees made project suggestions on a wiki and the organizers also set an optional challenge that involved working on specific React/React Native GitHub issues. For me the important part is not the specific project being worked on but the chance to work alongside someone you haven't met before and see how they think and use their tools. That's something I tend to miss out on working remotely.\n\nThe conference is of course about React and most of the talks are about React or related parts of the ecosystem. What interests me most though is the broader exploration around the benefits for users and developers of applying\nfunctional programming ideas to the front-end.\n\nI think it is because of the functional leaning that this conference and [Reactive\n2015](https://reactive2015.com/) last year attracted quite a number of people who work\nwith different but like-minded technologies (eg. Rust, Clojure, Elm). One fun hackathon output was [rust-redux](https://github.com/fanderzon/rust-redux)!\n\n## Talk Highlights\n\nThese are summaries of some of my favorite talks:\n\n### [Dan Abramov](https://twitter.com/dan_abramov) - [The Redux Journey](https://www.youtube.com/watch?v=uvAXVMwHJXU)\n\nDan gave a very good distillation of what made Redux as successful as it has been\nin a very short space of time. The first thing he pointed out is that it isn't because of\nits feature set - the library is tiny and a basic version of the code fits\non a couple of slides. However the constraints that it imposes on the application\nmake it easier to reason about what is going on, makes debugging and testing workflows\neasy and enables all kinds of interesting tooling which led to an ecosystem growing around it very quickly.\n\nThe part I found most interesting is that the design was ultimately a product of\ntwo 'stress tests' that had to be satisfied - the ability to reload the\napplication without losing state during development and the ability to do\ntime-travelling debugging. These two tests sound like cool albeit non-essential\nfeatures. It turns out though that a design which meets these needs also makes\na lot of other features with user or developer-facing value easy: Undo/redo,\noptimistic UI updates, persisting and restoring app state and cross-device\nsynchronization.\n\nWhilst this is all true, I think the success also owes much to more practical\nthings like very good documentation and the fact that there is enough\nflexibility and guidance on what to do when the app goes beyond the TodoMVC\nexample.\n\nOh, and there was a throwaway comment at the end that the \"time travelling\" requirement\ncame out of a change in the title of his talk last year at React Europe just to make it sound more\ninteresting ðŸ˜€. Richard Branson [would approve](http://www.goodreads.com/quotes/898551-if-somebody-offers-you-an-amazing-opportunity-but-you-are).\n\n### [Cheng Lou](https://twitter.com/_chenglou) - [The Spectrum of Abstraction](https://www.youtube.com/watch?v=mVVNJKv9esE)\n\nCheng's talk provided a useful way to evaluate technology choices in different\ncontexts by placing them on a spectrum of abstraction and understanding the\ntrade-offs that come with choosing technologies at different points on that\nspectrum. The \"spectrum of abstraction\" refers to the fact that some technologies\nsolve very specific problems (eg. The clock app on a phone) and others\nare much more abstract but are useful for a broader class of uses (eg. a Promises\nlibrary for JavaScript).\n\nCheng framed it as a kind of optimization problem to minimize the overall\ncognitive costs of the codebase in order to satisfy an evolving set of use\ncases. On the one hand, choosing unnecessary abstractions imposes a cognitive\ncost because of the gap between the abstraction and the problem being solved.\nOn the other hand choosing too many problem-specific tools imposes other costs.\n\nThere was an interesting look at the trade-offs around the power that different\nabstractions have. eg. Declarative systems for specifying things limit the\ndeveloper's freedom but often enable useful tooling and optimizations because\nof those constraints.\n\nThe talk then went on to discuss how to use this to deal with common\nchoices that front-end developers need to make. This included specific\nchoices such as imperative build systems (Gulp) vs declarative (Grunt) ones,\ntemplates vs. functions for transforming models into views, mutable vs. immutable data structures.\nIt also included broader questions of when to even use an abstraction versus\njust copying, pasting and tweaking code.\n\nI think having these kinds of frameworks for looking at problems is pretty handy, especially\nif you can visualize them, because then you can see gaps where there might\nbe solutions that have not been considered.\n\n### [Jeff Morrison](https://twitter.com/lbljeffmo) - [A deep dive into Flow](https://www.youtube.com/watch?v=VEaDsKyDxkY)\n\nThis was a technical dive into how the Flow type checker for JavaScript works,\nhow its approach differs from the type checking phase of a traditional compiler\nand the possibilities that opens up for detecting classes of bug that a typical\ncompiler might not be able to. For example, detecting all the places that unsanitized\nuser input might reach in a program via taint analysis.\n\nBeforehand I'd talked to many people who had some interest in trying type systems\nfor JavaScript and wanted recommendations for one or the other. Frankly I think both\nare good options that you can't go wrong with, but before this talk I only really\nunderstood the more practical surface-level issues (like platform support) rather\nthan the deeper ones that stem from the differing priorities of the projects\nand the fundamentally different ways that the tools understand code.\n\nA couple of times Jeff mentioned semantic differences between the type systems.\n[This presentation](http://djcordhose.github.io/flow-vs-typescript/2016_hhjs.html) from [Oliver Zeigermann](http://twitter.com/djcordhose)\nshows a couple of concrete examples (although note that the upcoming TypeScript 2.0 release\n_does_ have non-nullable types).\n\n### [Christopher Chedeau](https://twitter.com/Vjeux) - Being Successful at Open Source\n\nThe React community have produced several of the most successful projects on\nGitHub, where \"successful\" in this context was taken to mean widely adopted/having direct impact.\nThis talk provided specific advice for project maintainers on how to help their\nprojects gain traction. Many of the points were things that would be obvious to\nentrepreneurs behind new startups but are often forgotten in open source:\n\n 1. Talk to your users. It doesn't matter which channel you use\n    but you need to be present on them and active. In the React project,\n    different members (and active contributors) tended to focus on\n    specific channels (GitHub issues, StackOverflow).\n\n 2. Ask as many of your users as you can two specific questions:\n   1. **_What did you struggle with?_** - Find out what they had problems with,\n     aggregate the feedback and prioritize. The phrasing of this question\n     is important because you need to break the implicit social barrier\n     that prevents users from criticising your work.\n   1. **_What cool stuff are you building?_** - Show genuine interest in what your\n     users are doing. Ask them to share what they have been up to (eg. by writing blog\n     posts) and then promote their work.\n\n 3. Create the perception of a community. When evaluating a technology\n    choice, the perceived level of community activity is often an important\n    factor.\n    In the early days of a project, it helps to create this perception by\n    asking users to blog about what they have been doing, writing community round-ups etc.\n    Over time, this perceception becomes its own reality.\n\nSpeaking of making users feel loved, the organizers produced personalized\nT-shirts with GitHub handles for attendees who had made contributions to the\ncode or docs for various React-related projects. I thought that was a pretty\ncool touch. **Thank-you!**\n\n## Also worth watching\n\nPlus a couple of others from my notebook that I'd recommend seeing when\nthey are up on YouTube:\n\n* [Andrew Clark](https://twitter.com/acdlite)'s talk on recompose was a useful introduction to\n  the concept of higher order components, what you can do with them,\n  the performance costs they can impose and how to mitigate these.\n\n* [Lin Clark](https://twitter.com/linclark)'s [talk](https://www.youtube.com/watch?v=-t8eOoRsJ7M) was useful if you want to understand what actually happens\n  when a component is \"rendered\" and _why_ some of the performance tips\n  in the React documentation matter. Also it had cool cartoons.\n\n## Takeaways\n\n* The debate about basic app architecture from last year seems to have\n  largely been resolved in a pretty short time in favor of\n  the [Redux/Elm model](http://redux.js.org/docs/introduction/ThreePrinciples.html). [Lee Byron's talk](https://vimeo.com/album/3953264/video/166790294)\n  from RenderConf was cited a couple of times as a good overview of this architecture. The current\n  exploration has moved on to building all kinds of\n  useful tooling that assumes this model of state\n  management.\n\n* The exploration of how best to manage effects\n  nicely in Redux/Elm-like applications is ongoing\n  and there isn't a clear winner yet.\n\n* React Native has been around long enough now that\n  people outside of Facebook have been able to build and ship reasonably sizable applications\n  with it and provide a decent appraisal of where\n  it works well, where it still needs improvement\n  and where it isn't the right choice. [Brent Vatne's](https://twitter.com/notbrent) talk about\n  [li'st](https://li.st/) is worth watching.\n\n* One of the main React/React Native focus areas right now is performance\n  especially for the common use case of apps that need to render large lists of\n  complex _stuff_ whilst animating smoothly and responding instantly to user\n  input.\n"
    },
    {
      "metadata": {
        "slug": "webpack-dll-plugins",
        "title": "Optimizing Webpack build times and improving caching with DLL bundles",
        "date": "2016-02-06T00:00:00.000Z",
        "tags": [
          "webpack"
        ]
      },
      "body": "\nWebpack's `Dll` and `DllReference` plugins are a way to split\na large JavaScript project into multiple bundles which\ncan be compiled independently. They can be used to optimize\nbuild times (both full and incremental) and improve caching\nfor users by putting code which changes infrequently into\nseparate \"library\" bundles. The term 'Dll' is short for\n_Dynamically Linked Library_ which is a feature for native Windows\napplications that solves the same problem.\n\nFor example, suppose you have an application built with\nseveral large libraries or frameworks such as `jQuery`, `Angular`\nor `React` which change relatively infrequently, plus some utility code such as `underscore` or `lodash` which you change very rarely. The `Dll` plugins enable you to\nput the utility code in one bundle, the frameworks in another and your application\ncode in a separate one.\n\nThis has several benefits:\n\n * Your can compile the library bundles separately from the application bundle.\n   This means that during typical development, you would compile your library\n   bundles once and then run Webpack in `--watch` mode to recompile your application\n   bundles as you change your app. Since the application bundle is smaller,\n   the initial build will be faster and incremental builds will usually also\n   be quicker.\n * When your users load your app, they will only need to download\n   the bundles which have changed.\n * You can avoid cluttering up the configuration for your app bundle\n   with plugins/loaders etc. that are only needed for vendor bundles\n   or vice-versa. This helps with both build speed and maintainability.\n\nFor common versions of major libraries (eg. jQuery, Angular) you can also get\nthese benefits by consuming them from a CDN such as [cdnjs](https://cdnjs.com) using `<script>` tags and you should consider doing that.\n\nThe advantage of Webpack library bundles is that you can choose\nhow to assign modules to bundles and it works with libraries that\nyou consume from `npm` and `require()` from your app code.\n\n## Creating library bundles\n\nTo partition your code into library and application bundles, you will need to create two\nWebpack configurations, one that creates the library bundle(s) and another that\ncreates the application bundle(s) that use code from the libraries.\n\nIn the configuration for your library bundle, add `DllPlugin` to the list of plugins.\n`DllPlugin` does two things:\n\n 1. It exposes a `require()` function via a global variable on the page which\n    other bundles can use to require code from that bundle.\n\n 2. It generates a JSON manifest, which is a mapping between the file paths of\n    modules inside the bundle and the integer IDs that each module has been\n\tassigned. This manifest is used by your application bundles to determine\n\twhether a library bundle provides a particular module and if so, how\n\tto access it from another bundle.\n\n```js\n// vendor-bundles.webpack.config.js\nvar webpack = require('webpack')\n\nmodule.exports = {\n  entry: {\n    // create two library bundles, one with jQuery and\n    // another with Angular and related libraries\n    'jquery': ['jquery'],\n    'angular': ['angular', 'angular-router', 'angular-sanitize']\n  },\n\n  output: {\n    filename: '[name].bundle.js',\n    path: 'dist/',\n\n    // The name of the global variable which the library's\n    // require() function will be assigned to\n    library: '[name]_lib',\n  },\n\n  plugins: [\n    new webpack.DllPlugin({\n      // The path to the manifest file which maps between\n      // modules included in a bundle and the internal IDs\n      // within that bundle\n      path: 'dist/[name]-manifest.json',\n      // The name of the global variable which the library's\n      // require function has been assigned to. This must match the\n      // output.library option above\n      name: '[name]_lib'\n    }),\n  ],\n}\n```\n\nThe generated bundle will look something like this:\n\n```js\nvar angular_lib = (function (modules) {\n  // Bundle bootstrap code...\n})([\n  // Module 0 re-exports the require function,\n  // so that it becomes the value of angular_lib\n  function (module, exports, __webpack_require__) {\n    module.exports = __webpack_require__;\n  }\n])\n```\n\nAfter this is loaded, `window.angular_lib(<id>)` can be used by any\nother code on the page to require code from that bundle. Since modules use integer IDs which are internal to the bundle, you also need\nthe JSON manifest in order to map from file path to ID.\n\nThe JSON manifest is structured like this:\n\n```js\n{\n  \"name\": \"angular_lib\",\n  \"content\": {\n    \"./node_modules/angular/index.js\": 1,\n    ...\n  }\n}\n```\n\n## Creating application bundles\n\nIn the configuration for your application bundle, you will add a `DllRefrencePlugin`\ninstance to the list of plugins, one per library that you want to consume.\n\n`DllReferencePlugin` specifies the path of a manifest previously generated by\n`DllPlugin` to search for modules.\n\nWhen Webpack encounters a `require()` in your code, it will first check the\navailable DLLs to see if one provides that code. If it does, a stub module\ncontaining a reference to that DLL will be generated. Otherwise, the actual\ncode of the required module will be included in the application bundle as normal.\n\n```js\n// app.webpack.config.js\nvar webpack = require('webpack')\n\nmodule.exports = {\n  entry: {\n    app: './src/index'\n  },\n\n  plugins: [\n    new webpack.DllReferencePlugin({\n\t  context: '.',\n\t  manifest: require('./dist/jquery-manifest.json')\n\t}),\n\tnew webpack.DllReferencePlugin({\n\t  context: '.',\n\t  manifest: require('./dist/angular-manifest.json')\n\t}),\n  ]\n}\n```\n\nIf you look inside the generated `app.bundle.js`, you'll find code like this:\n\n```js\n// in your module\nvar angular = __webpack_require__(2)\n\n/* 2 */\n// get a reference to the bundle containing 'angular',\n// then require module (1) from that\nmodule.exports = (__webpack_require__(3))(1)\n\n/* 3 */\n// re-export the global variable created by the library\n// bundle\nmodule.exports = angular_lib\n```\n\nA minimal complete example can be found [in this Gist](https://gist.github.com/robertknight/058a194f45e77ff95fcd).\n\n\n## Comparison with other code splitting methods\n\nThe DLL plugins are not the only code-splitting mechanism available\nin Webpack. The other ones are:\n\n * [**CommonsChunkPlugin**](https://webpack.github.io/docs/list-of-plugins.html#commonschunkplugin) extracts out code that\n   is shared between multiple bundles and puts it into a separate\n   bundle. The advantage is that you don't need to specify which\n   code to put in the shared bundle, Webpack can automatically\n   identify that.\n\n   You can force specific libraries into a 'commons' chunk,\n   and this is a good solution if you only have a moderate amount\n   of vendor code.\n\n   The advantage is that you only need one Webpack configuration.\n   The disadvantage for large projects is that the 'commons'\n   chunk will be recompiled every time you run Webpack.\n\n * [**Code splitting**](https://webpack.github.io/docs/code-splitting.html) via `require.ensure()` allows lazy-loading\n   of chunks of code as particular pages or features are used.\n   This is useful to optimize the initial page load time by keeping\n   the main bundle small and then pulling in code for lesser-used\n   features on-demand.\n"
    },
    {
      "metadata": {
        "slug": "qt-widget-layout",
        "title": "Understanding the QWidget layout flow",
        "date": "2013-11-21T00:00:00.000Z",
        "tags": [
          "qt"
        ]
      },
      "body": "When layouts in a UI are not behaving as expected or performance is poor, it can be helpful to have a mental model of the layout process in order to know where to start debugging.  For web browsers there are some good resources which provide a description of the process at different levels. The layout documentation for [Qt](https://www.qt.io) describes the various layout facilities that are available but I haven't found a detailed description of the flow, so this is my attempt to explain what happens when a layout is triggered that ultimately ends up with the widgets being resized and repositioned appropriately.\n\n1. A widget's contents are modified in some way that require a layout update. Such changes can include:\n\t* Changes to the content of the widget (eg. the text in a label, content margins being altered)\n\t* Changes to the `sizePolicy()` of the widget\n\t* Changes to the layout() of the widget, such as new child widgets being added or removed\n    \n1. The widget calls `QWidget::updateGeometry()` which then performs several steps to trigger a layout:\n\t1. It invalidates any cached size information for the QWidgetItem associated with the widget in the parent layout.\n\t1. It recursively climbs up the widget tree (first to the parent widget, then the grandparent and so on), invalidating that widget's layout. The process stops when we reach a widget that is a top level window or doesn't have its own layout - we'll call this widget the top-level widget, though it might not actually be a window.\n\n1. If the top-level widget is not yet visible, then the process stops and layout is deferred until the widget is due to be shown.\n\n1. If the top-level widget is shown, a LayoutRequest event is posted asynchronously to the top-level widget, so a layout will be performed on the next pass through the event loop.\n\n1. If multiple layout requests are posted to the same top-level widget during a pass through the event loop, they will get compressed into a single layout request. This is similar to the way that multiple `QWidget::update()` requests are compressed into a single paint event.\n\n1. The top-level widget receives the LayoutRequest event on the next pass through the event loop. This can then be handled in one of two ways:\n\t1. If the widget has a layout, the layout will intercept the LayoutRequest event using an event filter and handle it by calling QLayout::activate()\n\t1. If the widget does not have a layout, it may handle the LayoutRequest event itself and manually set the geometry of its children.\n\n1. When the layout is activated, it first sets the fixed, minimum and/or maximum size constraints of the widget depending on QLayout::sizeConstraint(), using the values calculated by QLayout::minimumSize(), maximumSize() and sizeHint(). These functions will recursively proceed down the layout tree to determine the constraints for each item and produce a final size constraint for the whole layout.  This may or may not alter the current size of the widget.\n\n1.\tThe layout is then asked to resize its contents to fit the current size of the widget using QLayout::setGeometry(widget->size()). The specific implementation of the layout - whether it is a box layout, grid layout or something else then lays out its child items to fit this new size.\n\n1. For each item in the layout, the QLayoutItem::setGeometry() implementation will typically ask the item for various size parameters (minimum size, maximum size, size hint, height for width) and then decide upon a final size and position for the item. It will then invoke QLayoutItem::setGeometry() to update the position and size of the widget.\n\n1. If the layout item is itself a layout or a widget, steps 5-6 proceed recursively down the tree, updating all of the items whose constraints have been modified.\n"
    }
  ],
  "tags": {
    "conferences": [
      {
        "metadata": {
          "slug": "react-europe-2016",
          "title": "Notes on React Europe 2016",
          "date": "2016-06-04T00:00:00.000Z",
          "tags": [
            "conferences"
          ]
        },
        "body": "\nThe second [React Europe](https://www.react-europe.org/) conference has just finished and this is a jotting-down\nof some thoughts on the conference and my favorite talks whilst the Parisian red wine and almond croissants clear out of my system.\n\nThe organizers are kindly **[putting up videos on\nYouTube](https://www.youtube.com/channel/UCorlLn2oZfgOJ-FUcF2eZ1A)** as I write\nthis.\n\n## The Conference\n\nThis was a two-day conference preceded by a hackathon kindly hosted by Mozilla\nand a welcome event hosted by [Red Badger](https://twitter.com/redbadgerteam). It was well organized and ran smoothly, the main\npart consisted purely of 30 minute talks with a few lightning-talk spots. There\nwas plenty of time in-between for discussions.\n\nFor the hackathon attendees made project suggestions on a wiki and the organizers also set an optional challenge that involved working on specific React/React Native GitHub issues. For me the important part is not the specific project being worked on but the chance to work alongside someone you haven't met before and see how they think and use their tools. That's something I tend to miss out on working remotely.\n\nThe conference is of course about React and most of the talks are about React or related parts of the ecosystem. What interests me most though is the broader exploration around the benefits for users and developers of applying\nfunctional programming ideas to the front-end.\n\nI think it is because of the functional leaning that this conference and [Reactive\n2015](https://reactive2015.com/) last year attracted quite a number of people who work\nwith different but like-minded technologies (eg. Rust, Clojure, Elm). One fun hackathon output was [rust-redux](https://github.com/fanderzon/rust-redux)!\n\n## Talk Highlights\n\nThese are summaries of some of my favorite talks:\n\n### [Dan Abramov](https://twitter.com/dan_abramov) - [The Redux Journey](https://www.youtube.com/watch?v=uvAXVMwHJXU)\n\nDan gave a very good distillation of what made Redux as successful as it has been\nin a very short space of time. The first thing he pointed out is that it isn't because of\nits feature set - the library is tiny and a basic version of the code fits\non a couple of slides. However the constraints that it imposes on the application\nmake it easier to reason about what is going on, makes debugging and testing workflows\neasy and enables all kinds of interesting tooling which led to an ecosystem growing around it very quickly.\n\nThe part I found most interesting is that the design was ultimately a product of\ntwo 'stress tests' that had to be satisfied - the ability to reload the\napplication without losing state during development and the ability to do\ntime-travelling debugging. These two tests sound like cool albeit non-essential\nfeatures. It turns out though that a design which meets these needs also makes\na lot of other features with user or developer-facing value easy: Undo/redo,\noptimistic UI updates, persisting and restoring app state and cross-device\nsynchronization.\n\nWhilst this is all true, I think the success also owes much to more practical\nthings like very good documentation and the fact that there is enough\nflexibility and guidance on what to do when the app goes beyond the TodoMVC\nexample.\n\nOh, and there was a throwaway comment at the end that the \"time travelling\" requirement\ncame out of a change in the title of his talk last year at React Europe just to make it sound more\ninteresting ðŸ˜€. Richard Branson [would approve](http://www.goodreads.com/quotes/898551-if-somebody-offers-you-an-amazing-opportunity-but-you-are).\n\n### [Cheng Lou](https://twitter.com/_chenglou) - [The Spectrum of Abstraction](https://www.youtube.com/watch?v=mVVNJKv9esE)\n\nCheng's talk provided a useful way to evaluate technology choices in different\ncontexts by placing them on a spectrum of abstraction and understanding the\ntrade-offs that come with choosing technologies at different points on that\nspectrum. The \"spectrum of abstraction\" refers to the fact that some technologies\nsolve very specific problems (eg. The clock app on a phone) and others\nare much more abstract but are useful for a broader class of uses (eg. a Promises\nlibrary for JavaScript).\n\nCheng framed it as a kind of optimization problem to minimize the overall\ncognitive costs of the codebase in order to satisfy an evolving set of use\ncases. On the one hand, choosing unnecessary abstractions imposes a cognitive\ncost because of the gap between the abstraction and the problem being solved.\nOn the other hand choosing too many problem-specific tools imposes other costs.\n\nThere was an interesting look at the trade-offs around the power that different\nabstractions have. eg. Declarative systems for specifying things limit the\ndeveloper's freedom but often enable useful tooling and optimizations because\nof those constraints.\n\nThe talk then went on to discuss how to use this to deal with common\nchoices that front-end developers need to make. This included specific\nchoices such as imperative build systems (Gulp) vs declarative (Grunt) ones,\ntemplates vs. functions for transforming models into views, mutable vs. immutable data structures.\nIt also included broader questions of when to even use an abstraction versus\njust copying, pasting and tweaking code.\n\nI think having these kinds of frameworks for looking at problems is pretty handy, especially\nif you can visualize them, because then you can see gaps where there might\nbe solutions that have not been considered.\n\n### [Jeff Morrison](https://twitter.com/lbljeffmo) - [A deep dive into Flow](https://www.youtube.com/watch?v=VEaDsKyDxkY)\n\nThis was a technical dive into how the Flow type checker for JavaScript works,\nhow its approach differs from the type checking phase of a traditional compiler\nand the possibilities that opens up for detecting classes of bug that a typical\ncompiler might not be able to. For example, detecting all the places that unsanitized\nuser input might reach in a program via taint analysis.\n\nBeforehand I'd talked to many people who had some interest in trying type systems\nfor JavaScript and wanted recommendations for one or the other. Frankly I think both\nare good options that you can't go wrong with, but before this talk I only really\nunderstood the more practical surface-level issues (like platform support) rather\nthan the deeper ones that stem from the differing priorities of the projects\nand the fundamentally different ways that the tools understand code.\n\nA couple of times Jeff mentioned semantic differences between the type systems.\n[This presentation](http://djcordhose.github.io/flow-vs-typescript/2016_hhjs.html) from [Oliver Zeigermann](http://twitter.com/djcordhose)\nshows a couple of concrete examples (although note that the upcoming TypeScript 2.0 release\n_does_ have non-nullable types).\n\n### [Christopher Chedeau](https://twitter.com/Vjeux) - Being Successful at Open Source\n\nThe React community have produced several of the most successful projects on\nGitHub, where \"successful\" in this context was taken to mean widely adopted/having direct impact.\nThis talk provided specific advice for project maintainers on how to help their\nprojects gain traction. Many of the points were things that would be obvious to\nentrepreneurs behind new startups but are often forgotten in open source:\n\n 1. Talk to your users. It doesn't matter which channel you use\n    but you need to be present on them and active. In the React project,\n    different members (and active contributors) tended to focus on\n    specific channels (GitHub issues, StackOverflow).\n\n 2. Ask as many of your users as you can two specific questions:\n   1. **_What did you struggle with?_** - Find out what they had problems with,\n     aggregate the feedback and prioritize. The phrasing of this question\n     is important because you need to break the implicit social barrier\n     that prevents users from criticising your work.\n   1. **_What cool stuff are you building?_** - Show genuine interest in what your\n     users are doing. Ask them to share what they have been up to (eg. by writing blog\n     posts) and then promote their work.\n\n 3. Create the perception of a community. When evaluating a technology\n    choice, the perceived level of community activity is often an important\n    factor.\n    In the early days of a project, it helps to create this perception by\n    asking users to blog about what they have been doing, writing community round-ups etc.\n    Over time, this perceception becomes its own reality.\n\nSpeaking of making users feel loved, the organizers produced personalized\nT-shirts with GitHub handles for attendees who had made contributions to the\ncode or docs for various React-related projects. I thought that was a pretty\ncool touch. **Thank-you!**\n\n## Also worth watching\n\nPlus a couple of others from my notebook that I'd recommend seeing when\nthey are up on YouTube:\n\n* [Andrew Clark](https://twitter.com/acdlite)'s talk on recompose was a useful introduction to\n  the concept of higher order components, what you can do with them,\n  the performance costs they can impose and how to mitigate these.\n\n* [Lin Clark](https://twitter.com/linclark)'s [talk](https://www.youtube.com/watch?v=-t8eOoRsJ7M) was useful if you want to understand what actually happens\n  when a component is \"rendered\" and _why_ some of the performance tips\n  in the React documentation matter. Also it had cool cartoons.\n\n## Takeaways\n\n* The debate about basic app architecture from last year seems to have\n  largely been resolved in a pretty short time in favor of\n  the [Redux/Elm model](http://redux.js.org/docs/introduction/ThreePrinciples.html). [Lee Byron's talk](https://vimeo.com/album/3953264/video/166790294)\n  from RenderConf was cited a couple of times as a good overview of this architecture. The current\n  exploration has moved on to building all kinds of\n  useful tooling that assumes this model of state\n  management.\n\n* The exploration of how best to manage effects\n  nicely in Redux/Elm-like applications is ongoing\n  and there isn't a clear winner yet.\n\n* React Native has been around long enough now that\n  people outside of Facebook have been able to build and ship reasonably sizable applications\n  with it and provide a decent appraisal of where\n  it works well, where it still needs improvement\n  and where it isn't the right choice. [Brent Vatne's](https://twitter.com/notbrent) talk about\n  [li'st](https://li.st/) is worth watching.\n\n* One of the main React/React Native focus areas right now is performance\n  especially for the common use case of apps that need to render large lists of\n  complex _stuff_ whilst animating smoothly and responding instantly to user\n  input.\n"
      }
    ],
    "webpack": [
      {
        "metadata": {
          "slug": "webpack-dll-plugins",
          "title": "Optimizing Webpack build times and improving caching with DLL bundles",
          "date": "2016-02-06T00:00:00.000Z",
          "tags": [
            "webpack"
          ]
        },
        "body": "\nWebpack's `Dll` and `DllReference` plugins are a way to split\na large JavaScript project into multiple bundles which\ncan be compiled independently. They can be used to optimize\nbuild times (both full and incremental) and improve caching\nfor users by putting code which changes infrequently into\nseparate \"library\" bundles. The term 'Dll' is short for\n_Dynamically Linked Library_ which is a feature for native Windows\napplications that solves the same problem.\n\nFor example, suppose you have an application built with\nseveral large libraries or frameworks such as `jQuery`, `Angular`\nor `React` which change relatively infrequently, plus some utility code such as `underscore` or `lodash` which you change very rarely. The `Dll` plugins enable you to\nput the utility code in one bundle, the frameworks in another and your application\ncode in a separate one.\n\nThis has several benefits:\n\n * Your can compile the library bundles separately from the application bundle.\n   This means that during typical development, you would compile your library\n   bundles once and then run Webpack in `--watch` mode to recompile your application\n   bundles as you change your app. Since the application bundle is smaller,\n   the initial build will be faster and incremental builds will usually also\n   be quicker.\n * When your users load your app, they will only need to download\n   the bundles which have changed.\n * You can avoid cluttering up the configuration for your app bundle\n   with plugins/loaders etc. that are only needed for vendor bundles\n   or vice-versa. This helps with both build speed and maintainability.\n\nFor common versions of major libraries (eg. jQuery, Angular) you can also get\nthese benefits by consuming them from a CDN such as [cdnjs](https://cdnjs.com) using `<script>` tags and you should consider doing that.\n\nThe advantage of Webpack library bundles is that you can choose\nhow to assign modules to bundles and it works with libraries that\nyou consume from `npm` and `require()` from your app code.\n\n## Creating library bundles\n\nTo partition your code into library and application bundles, you will need to create two\nWebpack configurations, one that creates the library bundle(s) and another that\ncreates the application bundle(s) that use code from the libraries.\n\nIn the configuration for your library bundle, add `DllPlugin` to the list of plugins.\n`DllPlugin` does two things:\n\n 1. It exposes a `require()` function via a global variable on the page which\n    other bundles can use to require code from that bundle.\n\n 2. It generates a JSON manifest, which is a mapping between the file paths of\n    modules inside the bundle and the integer IDs that each module has been\n\tassigned. This manifest is used by your application bundles to determine\n\twhether a library bundle provides a particular module and if so, how\n\tto access it from another bundle.\n\n```js\n// vendor-bundles.webpack.config.js\nvar webpack = require('webpack')\n\nmodule.exports = {\n  entry: {\n    // create two library bundles, one with jQuery and\n    // another with Angular and related libraries\n    'jquery': ['jquery'],\n    'angular': ['angular', 'angular-router', 'angular-sanitize']\n  },\n\n  output: {\n    filename: '[name].bundle.js',\n    path: 'dist/',\n\n    // The name of the global variable which the library's\n    // require() function will be assigned to\n    library: '[name]_lib',\n  },\n\n  plugins: [\n    new webpack.DllPlugin({\n      // The path to the manifest file which maps between\n      // modules included in a bundle and the internal IDs\n      // within that bundle\n      path: 'dist/[name]-manifest.json',\n      // The name of the global variable which the library's\n      // require function has been assigned to. This must match the\n      // output.library option above\n      name: '[name]_lib'\n    }),\n  ],\n}\n```\n\nThe generated bundle will look something like this:\n\n```js\nvar angular_lib = (function (modules) {\n  // Bundle bootstrap code...\n})([\n  // Module 0 re-exports the require function,\n  // so that it becomes the value of angular_lib\n  function (module, exports, __webpack_require__) {\n    module.exports = __webpack_require__;\n  }\n])\n```\n\nAfter this is loaded, `window.angular_lib(<id>)` can be used by any\nother code on the page to require code from that bundle. Since modules use integer IDs which are internal to the bundle, you also need\nthe JSON manifest in order to map from file path to ID.\n\nThe JSON manifest is structured like this:\n\n```js\n{\n  \"name\": \"angular_lib\",\n  \"content\": {\n    \"./node_modules/angular/index.js\": 1,\n    ...\n  }\n}\n```\n\n## Creating application bundles\n\nIn the configuration for your application bundle, you will add a `DllRefrencePlugin`\ninstance to the list of plugins, one per library that you want to consume.\n\n`DllReferencePlugin` specifies the path of a manifest previously generated by\n`DllPlugin` to search for modules.\n\nWhen Webpack encounters a `require()` in your code, it will first check the\navailable DLLs to see if one provides that code. If it does, a stub module\ncontaining a reference to that DLL will be generated. Otherwise, the actual\ncode of the required module will be included in the application bundle as normal.\n\n```js\n// app.webpack.config.js\nvar webpack = require('webpack')\n\nmodule.exports = {\n  entry: {\n    app: './src/index'\n  },\n\n  plugins: [\n    new webpack.DllReferencePlugin({\n\t  context: '.',\n\t  manifest: require('./dist/jquery-manifest.json')\n\t}),\n\tnew webpack.DllReferencePlugin({\n\t  context: '.',\n\t  manifest: require('./dist/angular-manifest.json')\n\t}),\n  ]\n}\n```\n\nIf you look inside the generated `app.bundle.js`, you'll find code like this:\n\n```js\n// in your module\nvar angular = __webpack_require__(2)\n\n/* 2 */\n// get a reference to the bundle containing 'angular',\n// then require module (1) from that\nmodule.exports = (__webpack_require__(3))(1)\n\n/* 3 */\n// re-export the global variable created by the library\n// bundle\nmodule.exports = angular_lib\n```\n\nA minimal complete example can be found [in this Gist](https://gist.github.com/robertknight/058a194f45e77ff95fcd).\n\n\n## Comparison with other code splitting methods\n\nThe DLL plugins are not the only code-splitting mechanism available\nin Webpack. The other ones are:\n\n * [**CommonsChunkPlugin**](https://webpack.github.io/docs/list-of-plugins.html#commonschunkplugin) extracts out code that\n   is shared between multiple bundles and puts it into a separate\n   bundle. The advantage is that you don't need to specify which\n   code to put in the shared bundle, Webpack can automatically\n   identify that.\n\n   You can force specific libraries into a 'commons' chunk,\n   and this is a good solution if you only have a moderate amount\n   of vendor code.\n\n   The advantage is that you only need one Webpack configuration.\n   The disadvantage for large projects is that the 'commons'\n   chunk will be recompiled every time you run Webpack.\n\n * [**Code splitting**](https://webpack.github.io/docs/code-splitting.html) via `require.ensure()` allows lazy-loading\n   of chunks of code as particular pages or features are used.\n   This is useful to optimize the initial page load time by keeping\n   the main bundle small and then pulling in code for lesser-used\n   features on-demand.\n"
      }
    ],
    "qt": [
      {
        "metadata": {
          "slug": "qt-widget-layout",
          "title": "Understanding the QWidget layout flow",
          "date": "2013-11-21T00:00:00.000Z",
          "tags": [
            "qt"
          ]
        },
        "body": "When layouts in a UI are not behaving as expected or performance is poor, it can be helpful to have a mental model of the layout process in order to know where to start debugging.  For web browsers there are some good resources which provide a description of the process at different levels. The layout documentation for [Qt](https://www.qt.io) describes the various layout facilities that are available but I haven't found a detailed description of the flow, so this is my attempt to explain what happens when a layout is triggered that ultimately ends up with the widgets being resized and repositioned appropriately.\n\n1. A widget's contents are modified in some way that require a layout update. Such changes can include:\n\t* Changes to the content of the widget (eg. the text in a label, content margins being altered)\n\t* Changes to the `sizePolicy()` of the widget\n\t* Changes to the layout() of the widget, such as new child widgets being added or removed\n    \n1. The widget calls `QWidget::updateGeometry()` which then performs several steps to trigger a layout:\n\t1. It invalidates any cached size information for the QWidgetItem associated with the widget in the parent layout.\n\t1. It recursively climbs up the widget tree (first to the parent widget, then the grandparent and so on), invalidating that widget's layout. The process stops when we reach a widget that is a top level window or doesn't have its own layout - we'll call this widget the top-level widget, though it might not actually be a window.\n\n1. If the top-level widget is not yet visible, then the process stops and layout is deferred until the widget is due to be shown.\n\n1. If the top-level widget is shown, a LayoutRequest event is posted asynchronously to the top-level widget, so a layout will be performed on the next pass through the event loop.\n\n1. If multiple layout requests are posted to the same top-level widget during a pass through the event loop, they will get compressed into a single layout request. This is similar to the way that multiple `QWidget::update()` requests are compressed into a single paint event.\n\n1. The top-level widget receives the LayoutRequest event on the next pass through the event loop. This can then be handled in one of two ways:\n\t1. If the widget has a layout, the layout will intercept the LayoutRequest event using an event filter and handle it by calling QLayout::activate()\n\t1. If the widget does not have a layout, it may handle the LayoutRequest event itself and manually set the geometry of its children.\n\n1. When the layout is activated, it first sets the fixed, minimum and/or maximum size constraints of the widget depending on QLayout::sizeConstraint(), using the values calculated by QLayout::minimumSize(), maximumSize() and sizeHint(). These functions will recursively proceed down the layout tree to determine the constraints for each item and produce a final size constraint for the whole layout.  This may or may not alter the current size of the widget.\n\n1.\tThe layout is then asked to resize its contents to fit the current size of the widget using QLayout::setGeometry(widget->size()). The specific implementation of the layout - whether it is a box layout, grid layout or something else then lays out its child items to fit this new size.\n\n1. For each item in the layout, the QLayoutItem::setGeometry() implementation will typically ask the item for various size parameters (minimum size, maximum size, size hint, height for width) and then decide upon a final size and position for the item. It will then invoke QLayoutItem::setGeometry() to update the position and size of the widget.\n\n1. If the layout item is itself a layout or a widget, steps 5-6 proceed recursively down the tree, updating all of the items whose constraints have been modified.\n"
      }
    ]
  }
}